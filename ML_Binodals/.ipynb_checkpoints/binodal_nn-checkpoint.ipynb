{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe34f7c-2f77-43bb-892a-4fa3471091b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version = 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tensorflow              import keras\n",
    "from keras                   import layers\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "print(f\"tensorflow version = {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c479bc7b-2466-4d40-817a-f52d244339d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the trimmed binodals\n",
    "current_directory = os.getcwd()\n",
    "all_files         = os.listdir(current_directory)\n",
    "trimmed_files     = [file for file in all_files if (file.startswith(\"trim\") and file.endswith(\".binodal\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a77dcca-e037-40da-ae82-ae071029a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all the trimmed files and get them as dataframes\n",
    "the_output_binodals = []\n",
    "for tfile in trimmed_files:\n",
    "    df = pd.read_csv(tfile, sep='\\|', names=[\"phi_s_top\",\"phi_p_top\",\"phi_c_top\",\"phi_s_bot\",\"phi_p_bot\",\"phi_c_bot\"], skiprows=1, engine='python')\n",
    "    data = df.values.reshape(40000, 6)\n",
    "\n",
    "    # convery to numpy object\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "    the_output_binodals.append(data)\n",
    "    \n",
    "the_outputs = np.array(the_output_binodals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dfea1b-a5d3-464d-9f3e-85aebe56db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all the trimmed files and get the inputs as a vector\n",
    "# this will be a vector that looks like [vs, vp, vc, chi_sc, chi_ps, chi_pc]\n",
    "\n",
    "# define a regex that captures numerical, positive and negative, decimal values\n",
    "pattern = r'(?:vs|vc|vp|chisc|chips|chipc)_([-+]?\\d+\\.\\d+)'\n",
    "the_inputs = []\n",
    "for tstring in trimmed_files:\n",
    "    \n",
    "    # find all matches\n",
    "    matches = re.findall(pattern, tstring)\n",
    "    the_inputs.append(np.array(matches, dtype=np.float64))\n",
    "    \n",
    "the_inputs = np.array(the_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badfb776-981f-42d3-8401-361417415d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure these are equal: the number of inputs points is 100, and number of outputs is 100.\n",
      "They are equal! Moving on...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 22:14:04.517494: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# tensorflowify everything\n",
    "the_inputs  = tf.constant(the_inputs)\n",
    "the_outputs = tf.constant(the_output_binodals)\n",
    "\n",
    "print(f\"Make sure these are equal: the number of inputs points is {the_inputs.shape[0]}, and number of outputs is {the_outputs.shape[0]}.\")\n",
    "if the_inputs.shape[0]==the_outputs.shape[0]:\n",
    "    print (\"They are equal! Moving on...\")\n",
    "else:\n",
    "    print (\"There is a problem. Exiting...\")\n",
    "    exit  ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3818167e-9d83-434f-b0b4-af110e536210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total size of the dataset\n",
    "total_samples    = the_inputs.shape[0]\n",
    "\n",
    "# get the split\n",
    "train_percentage = 0.8\n",
    "\n",
    "# calculate the number of samples for training and testing\n",
    "train_size = int(total_samples * train_percentage)\n",
    "test_size  = total_samples - train_size\n",
    "\n",
    "# create a tensorflow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices ((the_inputs, the_outputs))\n",
    "\n",
    "# shuffle and split the dataset\n",
    "dataset       = dataset.shuffle(total_samples, seed=42)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset  = dataset.skip(train_size)\n",
    "\n",
    "# split these into train_inputs, train_outputs, and test_inputs, test_outputs\n",
    "# Assuming your dataset consists of (input, output) pairs\n",
    "def map_func(input, output):\n",
    "    return input, output\n",
    "\n",
    "# Apply the map function to the training dataset to get train_inputs and train_outputs\n",
    "train_dataset = train_dataset.map(map_func)\n",
    "\n",
    "# Apply the map function to the test dataset to get test_inputs and test_outputs\n",
    "test_dataset = test_dataset.map(map_func)\n",
    "\n",
    "# Now, create separate lists or arrays for train_inputs, train_outputs, test_inputs, and test_outputs\n",
    "train_inputs, train_outputs = [], []\n",
    "test_inputs, test_outputs = [], []\n",
    "\n",
    "for input, output in train_dataset:\n",
    "    train_inputs.append(input)\n",
    "    train_outputs.append(output)\n",
    "\n",
    "for input, output in test_dataset:\n",
    "    test_inputs.append(input)\n",
    "    test_outputs.append(output)\n",
    "\n",
    "test_inputs   = tf.convert_to_tensor(test_inputs)\n",
    "test_outputs  = tf.convert_to_tensor(test_outputs)\n",
    "\n",
    "train_inputs  = tf.convert_to_tensor(train_inputs)\n",
    "train_outputs = tf.convert_to_tensor(train_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd10675-b713-42cc-bfbd-990f1cbb42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start creating the structure of the neural network\n",
    "inputs = keras.Input(shape=(6,))\n",
    "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(40000*6, activation=\"linear\")(x)\n",
    "outputs = layers.Reshape((40000, 6))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf5bf56-5efc-4a55-b5b3-6b0e4193e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the skeleton of our neural network we need to define a custom loss function\n",
    "# we will try to faithfully recreate the ones in the training set, but also \n",
    "# tack on the loss that comes with getting the chemical potentials wrong\n",
    "mu_a = lambda phi_a, phi_b, phi_c, vs, vc, vp, chi_sc, chi_ps, chi_pc: tf.math.log(phi_a) + 1 - phi_a \\\n",
    "        - vs/vp * phi_b - vs/vc * (phi_c) + vs * (phi_b**2 * chi_ps + (phi_c)**2 * \\\n",
    "        chi_sc + phi_b * (phi_c) * (chi_ps + chi_sc - chi_pc) ) \n",
    "\n",
    "mu_b = lambda phi_a, phi_b, phi_c, vs, vc, vp, chi_sc, chi_ps, chi_pc: tf.math.log(phi_b) + 1 - phi_b \\\n",
    "        - vp/vs * phi_a - vp/vc * (phi_c) + vp * (phi_a**2 * chi_ps + (phi_c)**2 * \\\n",
    "        chi_pc + phi_a * (phi_c) * (chi_ps + chi_pc - chi_sc) )\n",
    "\n",
    "mu_c = lambda phi_a, phi_b, phi_c, vs, vc, vp, chi_sc, chi_ps, chi_pc: tf.math.log(phi_c) + 1 - phi_c \\\n",
    "        - vc/vs * phi_a - vc/vp * phi_b + vc * (phi_a**2 * chi_sc + phi_b**2 * \\\n",
    "        chi_pc + phi_a * phi_b * (chi_sc + chi_pc - chi_ps) )\n",
    "\n",
    "delta_mu_a = lambda pa1, pb1, pc1, pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc: \\\n",
    "tf.math.abs(mu_a(pa1, pb1, pc1, vs, vc, vp, chi_sc, chi_ps, chi_pc) - \\\n",
    "mu_a(pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc))\n",
    "\n",
    "delta_mu_b = lambda pa1, pb1, pc1, pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc: \\\n",
    "tf.math.abs(mu_b(pa1, pb1, pc1, vs, vc, vp, chi_sc, chi_ps, chi_pc) - \\\n",
    "mu_b(pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc))\n",
    "\n",
    "delta_mu_c = lambda pa1, pb1, pc1, pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc: \\\n",
    "tf.math.abs(mu_c(pa1, pb1, pc1, vs, vc, vp, chi_sc, chi_ps, chi_pc) - \\\n",
    "mu_c(pa2, pb2, pc2, vs, vc, vp, chi_sc, chi_ps, chi_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dae120-2a0f-4c7f-9d22-89d68abd2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuLoss(Loss):\n",
    "    def __init__(self, name=\"mu_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        feature_inputs = inputs  # Access the input layer directly\n",
    "\n",
    "        loss_dmu_a = tf.reduce_sum(tf.abs(delta_mu_a (y_pred[:, 0], y_pred[:,1], y_pred[:,2], y_pred[:,3], y_pred[:,4], y_pred[:,5], \\\n",
    "                                                      feature_inputs[0], feature_inputs[1], feature_inputs[2], feature_inputs[3], \\\n",
    "                                                      feature_inputs[4], feature_inputs[5])))\n",
    "        \n",
    "        loss_dmu_b = tf.reduce_sum(tf.abs(delta_mu_b (y_pred[:, 0], y_pred[:,1], y_pred[:,2], y_pred[:,3], y_pred[:,4], y_pred[:,5], \\\n",
    "                                                      feature_inputs[0], feature_inputs[1], feature_inputs[2], feature_inputs[3], \\\n",
    "                                                      feature_inputs[4], feature_inputs[5])))\n",
    "\n",
    "        loss_dmu_c = tf.reduce_sum(tf.abs(delta_mu_c (y_pred[:, 0], y_pred[:,1], y_pred[:,2], y_pred[:,3], y_pred[:,4], y_pred[:,5], \\\n",
    "                                                      feature_inputs[0], feature_inputs[1], feature_inputs[2], feature_inputs[3], \\\n",
    "                                                      feature_inputs[4], feature_inputs[5])))\n",
    "\n",
    "        mu_loss  = loss_dmu_a + loss_dmu_b + loss_dmu_c\n",
    "        mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "        loss     = mu_loss + mse_loss\n",
    "        return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d217fb-f5d7-49e3-b15a-7ccec3b2e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the inputs and outputs\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4212d6-d58c-48c2-a236-6722f09a77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the thing before we can fit things to it\n",
    "model.compile (optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f7e1a-2274-47c5-b2be-5700bcd20274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 22:14:09.634614: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 3s 650ms/step - loss: 0.1643 - mean_squared_error: 0.1643 - val_loss: 0.1140 - val_mean_squared_error: 0.1140\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 0.1136 - mean_squared_error: 0.1136 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0528 - mean_squared_error: 0.0528 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 271ms/step - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 2s 481ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 275ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0138 - val_mean_squared_error: 0.0138\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 1s 389ms/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 2s 464ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 1s 429ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 1s 283ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 1s 433ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 1s 289ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 2s 568ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 1s 417ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 1s 271ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 1s 437ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 1s 411ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 1s 411ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 36/100\n"
     ]
    }
   ],
   "source": [
    "# allow it to fit\n",
    "model.fit(train_inputs, train_outputs, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624c35-ec19-4789-88d8-4285a729280d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db402c4-d35c-45ec-a936-1e8e769b924d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb878324-04a5-454d-a144-26ee63d7c4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba0dbd-c3ea-470d-a721-13d893dd952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726d3d1-8936-4f2c-9591-620b10a2c9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GBCG",
   "language": "python",
   "name": "gbcg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
