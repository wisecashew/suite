#!/home/satyend/.conda/envs/data_analysis/bin/python

import numpy as np 
import re 
import matplotlib
matplotlib.use('Agg')
import matplotlib.cm as cm
import matplotlib.pyplot as plt 
import pandas as pd
import os
import aux 
import time 
import sys 
import multiprocessing 
import itertools
from sklearn.linear_model import LinearRegression 

os.system("taskset -p 0xfffff %d" % os.getpid())
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'

sys.stdout.flush() 

'''
This code will take in a trajectory file generated by my MonteCarlo engine and 
gives you the flory exponent
'''
''' 
shebang for cluster: #!/usr/licensed/anaconda3/2020.7/bin/python
shebang for homemachine: #!/usr/bin/env python3
'''


import argparse 
parser = argparse.ArgumentParser(description="Read a trajectory file and obtain the flory exponent from that file.")
parser.add_argument('-dop', metavar='DOP', dest='dop', type=int, action='store', help='enter a degree of polymerization.')
parser.add_argument('-s', metavar='S', type=int, dest='s', action='store', help='start parsing after this move number (not index or line number in file).', default=100)
parser.add_argument('--coords', dest='c', metavar='coords.txt', action='store', type=str, help='Name of energy dump file to parse information.', default='coords.txt')
parser.add_argument('-nproc', metavar='N', type=int, dest='nproc', action='store', help='Request these many proccesses.')
parser.add_argument('-d1', dest='d1', metavar='d1', action='store', type=int, help='Starting index.')
parser.add_argument('-d2', dest='d2', metavar='d2', action='store', type=int, help='End index.')
args = parser.parse_args() 

divnorm = matplotlib.colors.SymLogNorm (0.005, vmin=-0.1, vmax=0.1)


def get_starting_ind ( U, T, num, dop, dumpfile):
    filename = U + "/DOP_" + str(dop) + "/" + str(T) + "/" + dumpfile + "_" + str(num) + ".mc"
    df = pd.read_csv(filename, sep=' \| ', names=["energy", "mm_tot", "mm_aligned", "mm_naligned", "ms1_tot", "ms1_aligned", "ms1_naligned", "ms2_tot", "ms2_aligned", "ms2_naligned", "ms1s2_tot",  "ms1s2_aligned", "ms1s2_naligned", "time_step"], engine='python', skiprows=0)
    L = len(df["energy"])

    return int(df["time_step"].values[L-3000])



def get_flory (U, T, num, dop, coords_file, starting_index, d1, d2):
    x = list (np.arange(d1, d2+1))
    y = []
    starting_index = get_starting_ind (U, T, num, dop, "energydump")
    for delta in x:
        y.append ( aux.single_sim_flory_exp (U, T, num, dop, coords_file, starting_index, delta ) )

    x = np.asarray (np.log(x)).reshape((-1,1))
    y = np.asarray (np.log(y))
    print (y, flush=True)
    model = LinearRegression()
    model.fit(x, y)
    r2 = model.score (x, y)
    # print ("slope = ",model.coef_)
    return (model.coef_, r2)


if __name__ == "__main__":

    start = time.time() 
    ##################################

    U_list = aux.dir2U ( os.listdir (".") )
    # U_list = ["U1", "U9"]
    DB_DICT = {} 
    DB_DICT["U"]  = []
    DB_DICT["T"]  = []
    DB_DICT["d1"] = []
    DB_DICT["d2"] = []
    DB_DICT["nu_mean"] = []
    DB_DICT["nu_err" ] = []
    DB_DICT["nu_r2"  ] = []
    flory_dict    = {}
    r2_dict       = {} 
    ntraj_dict = {}
    dop            = args.dop
    coords_files   = args.c
    starting_index = args.s
    nproc          = args.nproc
    
    fig = plt.figure( figsize=(8,6) )
    ax  = plt.axes() 
    ax.tick_params(direction='in', bottom=True, top=True, left=True, right=True, which='both')
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)
    i = 0

    # instantiating pool
    pool1 = multiprocessing.Pool ( processes=nproc )
    
    pool_list = [pool1]
    
    temperatures = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.5, 5.0, 10.0, 25.0, 50.0, 100.0]
    for temp in temperatures:
        # print ( "Inside U = " + U + ", and N = " + str(dop) + "...", flush=True )
        
        frac_list = [] 
        # get num_list for each temperature
        master_U_list = []
        master_num_list = []
        flory_mean = []
        flory_err  = []
        flory_r2   = []
        flory_dict.clear()
        r2_dict.clear()
        ntraj_dict.clear()
        for U in U_list:
            # print ("T is " + str(T), flush=True)
            frac_list.append ( aux.get_frac (str(U)+"/geom_and_esurf.txt") )
            num_list = list(np.unique ( aux.dir2nsim (os.listdir (str(U) + "/DOP_" + str(dop) + "/" + str(temp) ) ) ) )
            master_num_list.extend ( num_list )
            master_U_list.extend ( [U]*len( num_list ) )
            ntraj_dict[U] = len ( num_list )
            flory_dict[U] = []
            r2_dict   [U] = []

        # start multiprocessing... keeping in mind that each node only has 96 cores
        # start splitting up master_num_list and master_temp_list
        idx_range = len (master_num_list)//nproc + 1
        for uidx in range(idx_range):
            if uidx == idx_range-1:
                results = pool_list[ 0 ] .starmap ( get_flory, zip( master_U_list[uidx*nproc:], itertools.repeat(temp), master_num_list[uidx*nproc:], itertools.repeat(dop), itertools.repeat(coords_files), itertools.repeat(starting_index), itertools.repeat(args.d1), itertools.repeat(args.d2) ) )
            else:
                results = pool_list[ 0 ] .starmap ( get_flory, zip( master_U_list[uidx*nproc:(uidx+1)*nproc], itertools.repeat(temp), master_num_list[uidx*nproc:(uidx+1)*nproc], itertools.repeat(dop), itertools.repeat(coords_files), itertools.repeat(starting_index), itertools.repeat(args.d1), itertools.repeat(args.d2) ) )
                
            print ("Pool has been closed. This pool has {} threads.".format (len(results) ), flush=True )
            for k in range(len(master_U_list[uidx*nproc:(uidx+1)*nproc])):
                flory_dict[master_U_list[uidx*nproc+k]].append ( results[k][0] )
                r2_dict   [master_U_list[uidx*nproc+k]].append ( results[k][1] )


        for U in U_list:
            flory_mean.append ( np.mean ( flory_dict[U] ) )
            flory_err.append  ( np.std  ( flory_dict[U] ) / np.sqrt ( ntraj_dict[U] ) )
            flory_r2.append   ( np.mean ( r2_dict[U] ) )
        
        DB_DICT["U"].extend       (U_list)
        DB_DICT["T"].extend       ([temp]*len(flory_mean))
        DB_DICT["d1"].extend      ([args.d1]*len(flory_mean))
        DB_DICT["d2"].extend      ([args.d2]*len(flory_mean))
        DB_DICT["nu_mean"].extend (flory_mean)
        DB_DICT["nu_err"].extend  (flory_err)
        DB_DICT["nu_r2"].extend   (flory_r2)

    pool1.close()
    pool1.join()
    
    df = pd.DataFrame.from_dict (DB_DICT)
    df.to_csv ("FLORY-INFO-"+str(args.d1)+"-"+str(args.d2)+".mc", sep='|', index=False)
    
    stop = time.time()
    
    print ("Run time for N = " + str(args.dop) + " is {:.2f} seconds.".format(stop-start), flush=True)
    
