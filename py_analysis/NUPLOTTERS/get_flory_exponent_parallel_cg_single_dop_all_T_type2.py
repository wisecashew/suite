#!/home/satyend/.conda/envs/data_analysis/bin/python

import numpy as np 
import re 
import matplotlib
matplotlib.use('Agg')
import matplotlib.cm as cm
import matplotlib.pyplot as plt 
import pandas as pd
import os
import time 
import sys 
sys.path.insert(0, '/scratch/gpfs/satyend/MC_POLYMER/polymer_lattice/lattice_md/py_analysis')
import aux 
import multiprocessing 
import itertools
from sklearn.linear_model import LinearRegression 
from sklearn.linear_model import HuberRegressor

os.system("taskset -p 0xfffff %d" % os.getpid())
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'

sys.stdout.flush() 

'''
This code will take in a trajectory file generated by my MonteCarlo engine and 
gives you the flory exponent
'''
''' 
shebang for cluster: #!/usr/licensed/anaconda3/2020.7/bin/python
shebang for homemachine: #!/usr/bin/env python3
'''


import argparse 
parser = argparse.ArgumentParser(description="Read a trajectory file and obtain the flory exponent from that file.")
parser.add_argument('-dop', metavar='dop', type=int, dest='dop', action='store', help='Enter degree of polymerization')
parser.add_argument('--coords', dest='c', metavar='coords.txt', action='store', type=str, help='Name of energy dump file to parse information.', default='coords.txt')
parser.add_argument('-nproc', metavar='N', type=int, dest='nproc', action='store', help='Request these many proccesses.')
parser.add_argument('-d1', dest='d1', metavar='d1', action='store', type=int, help='Starting index.')
parser.add_argument('-d2', dest='d2', metavar='d2', action='store', type=int, help='End index.')
args = parser.parse_args() 

divnorm = matplotlib.colors.SymLogNorm (0.005, vmin=-0.1, vmax=0.1)


def get_starting_ind (T, model, num, dumpfile):
	filename = str(T) + "/FORM1/MODEL" + str(model) + "/" + dumpfile + "_" + str(num) + ".mc"
	df = pd.read_csv(filename, sep='\|', names=["energy", "mm_tot", "ms_tot", "time_step"], engine='python', skiprows=0)
	# print (df)
	L = len(df["energy"])
	
	return int(df["time_step"].values[L-2000])

def single_sim_flory_exp ( T, model, num, coords_file, starting_index, delta, dop ):

	filename = str(T)+"/FORM1/MODEL"+str(model)+"/"+coords_file+"_"+str(num)+".mc"
	edge     = aux.edge_length (dop)
	master_dict  = aux.get_pdict( filename, starting_index, dop, edge, edge, edge)
	offset_list  = []

	for key in master_dict:
		coord_arr    = aux.unfuck_polymer ( master_dict[key][0], edge, edge, edge )
		delta_coords = coord_arr [0:dop-delta] - coord_arr [delta:]
		offset = list(np.linalg.norm ( delta_coords, axis=1 ) **2 )
		offset_list.extend(offset)

	return np.mean (offset_list)


def get_avg_flory ( T, model, num, coords_file, d1, d2, dop):

	x = list ( np.arange (d1, d2+1))
	y = []
	starting_index = get_starting_ind (T, model, num, "energydump")
	for delta in x:
		y.append (single_sim_flory_exp (T, model, num, coords_file, starting_index, delta, dop) )
	nu = []
	y = np.array (np.log(y))
	x = np.array (np.log(x)).reshape((-1,1))
	for j in range (1, len(x)-1):
		model = HuberRegressor()
		model.fit (x[j-1:j+2], y[j-1:j+2])
		r2 = model.score (x[j-1:j+2], y[j-1:j+2])
		nu.append (model.coef_[0])

	return np.mean (nu)

if __name__ == "__main__":

	start = time.time() 

	T_list    = [0.01, 0.02, 0.05, 0.1, 0.3, 0.5, 1.0, 2.5, 5.0, 10.0, 17.5, 25.0, 50.0, 100.0]
	DB_DICT   = {}
	DB_DICT["T"]  = []
	DB_DICT["d1"] = []
	DB_DICT["d2"] = []
	DB_DICT["nu_mean"] = []
	DB_DICT["nu_err"  ] = []
	dop            = args.dop
	coords_file    = args.c
	nproc          = args.nproc
	i = 0

	ntraj_dict  = {}
	nmodel_dict = {}
	y_dict      = {}
	for T in T_list:
		nmodel_dict[T] = np.max (  aux.dir2nmodel( os.listdir (str(T) + "/FORM1/.") ) ) - 1


	# instantiating pool
	pool1 = multiprocessing.Pool ( processes=nproc )

	
	master_temp_list = []
	master_num_list  = []
	master_nmod_list = []
	flory_mean       = []
	flory_err        = []
	
	for T in T_list:
		num_list = list (np.unique (aux.dir2nsim (os.listdir (str(T)+"/FORM1/MODEL"+str(nmodel_dict[T])) ) ) )
		master_num_list.extend  (num_list)
		master_temp_list.extend ([T] * len (num_list) )
		master_nmod_list.extend ([nmodel_dict[T]] * len (num_list) )
		ntraj_dict[T] = len( num_list )
		y_dict[T] = []


	print ("master_num_list  = ",master_num_list)
	print ("master_temp_list = ",master_temp_list)
	print ("master_nmod_list = ",master_nmod_list)

	# start parallel computation... keeping in mind that each node only has 96 cores 
	# start splitting up master_num_list and master_temp_list
	idx_range = len (master_num_list) // nproc + 1

	for uidx in range (idx_range):
		if uidx == idx_range - 1:
			results = pool1.starmap ( get_avg_flory, zip (master_temp_list[uidx*nproc:], master_nmod_list[uidx*nproc:], master_num_list[uidx*nproc:], itertools.repeat (coords_file), itertools.repeat(args.d1), itertools.repeat (args.d2), itertools.repeat(dop) ) )
		else:
			results = pool1.starmap ( get_avg_flory, zip (master_temp_list[uidx*nproc:(uidx+1)*nproc], master_nmod_list[uidx*nproc:(uidx+1)*nproc], master_num_list[uidx*nproc:(uidx+1)*nproc], itertools.repeat (coords_file), itertools.repeat (args.d1), itertools.repeat (args.d2), itertools.repeat(dop) ) )

		print ("Pool has been closed. This pool has {} threads.".format (len(results) ), flush=True)
		for k in range (len (master_temp_list[uidx*nproc:(uidx+1)*nproc]) ):
			y_dict[master_temp_list[uidx*nproc+k]].append (results[k])


	for T in np.unique (master_temp_list):
		DB_DICT["T"].append (T)
		DB_DICT["d1"].append (args.d1)
		DB_DICT["d2"].append (args.d2)
		DB_DICT["nu_mean"].append (np.mean (y_dict[T]) )
		DB_DICT["nu_err"].append (np.std (y_dict[T]) / np.sqrt (len (y_dict[T]) ) )


	pool1.close()
	pool1.join ()

	df = pd.DataFrame.from_dict (DB_DICT)
	df.to_csv ("FLORY-EXPONENTS-FORM1-"+str(args.d1)+"-"+str(args.d2)+".mc", sep='|', index=False)

	stop = time.time ()
	
	print ("Run time for N = " + str(args.dop) + " is {:.2f} seconds.".format (stop-start), flush=True)
