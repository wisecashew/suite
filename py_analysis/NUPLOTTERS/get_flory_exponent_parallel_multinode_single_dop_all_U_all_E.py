#!/home/satyend/.conda/envs/phase/bin/python

import numpy as np 
import re 
import matplotlib
matplotlib.use('Agg')
import matplotlib.cm as cm
import matplotlib.pyplot as plt 
import pandas as pd
import os
# import aux 
import time 
import sys 
sys.path.insert(0, '/scratch/gpfs/satyend/MC_POLYMER/polymer_lattice/lattice_md/py_analysis')
import aux 
import multiprocessing 
import itertools
from sklearn.linear_model import LinearRegression 
from sklearn.linear_model import HuberRegressor

os.system("taskset -p 0xfffff %d" % os.getpid())
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'

sys.stdout.flush() 

'''
This code will take in a trajectory file generated by my MonteCarlo engine and 
gives you the flory exponent
'''
''' 
shebang for cluster: #!/usr/licensed/anaconda3/2020.7/bin/python
shebang for homemachine: #!/usr/bin/env python3
'''


import argparse 
parser = argparse.ArgumentParser(description="Read a trajectory file and obtain the flory exponent from that file.")
parser.add_argument('-dop', metavar='DOP', dest='dop', type=int, action='store', help='enter a degree of polymerization.')
parser.add_argument('--coords', dest='c', metavar='coords.txt', action='store', type=str, help='Name of energy dump file to parse information.', default='coords.txt')
parser.add_argument('--Hmix', dest='Hmix', action='store', nargs='+', type=str, help='List of potential energy surfaces to probe.')
parser.add_argument('--set', dest='set', action='store', type=str, help='List of potential energy surfaces to probe.')
parser.add_argument('-nproc', metavar='N', type=int, dest='nproc', action='store', help='Request these many proccesses.')
parser.add_argument('-d1', dest='d1', metavar='d1', action='store', type=int, help='Starting index.')
parser.add_argument('-d2', dest='d2', metavar='d2', action='store', type=int, help='End index.')
args = parser.parse_args() 

divnorm = matplotlib.colors.LogNorm (vmin=-0.01, vmax=100)
cmap = cm.coolwarm

def get_starting_ind ( U, mixH, num, dop, dumpfile):
    filename = U + "/DOP_" + str(dop) + "/" + mixH + "/" + dumpfile + "_" + str(num) + ".mc"
    df = pd.read_csv(filename, sep=' \| ', names=["energy", "mm_tot", "mm_aligned", "mm_naligned", "ms1_tot", "ms1_aligned", "ms1_naligned", "ms2_tot", "ms2_aligned", "ms2_naligned", "ms1s2_tot",  "ms1s2_aligned", "ms1s2_naligned", "time_step"], engine='python', skiprows=0)
    L = len(df["energy"])

    return int(df["time_step"].values[L-3000])


def get_avg_amounts (U, mixH, num, dop, coords_file, starting_index, d1, d2):
	x = list (np.arange (d1, d2+1))
	y = []
	starting_index = get_starting_ind (U, mixH, num, dop, "energydump")
	for delta in x:
		y.append ( aux.single_sim_flory_exp (U, mixH, num, dop, coords_file, starting_index, delta) )

	return np.array (y)


if __name__ == "__main__":

    start = time.time() 
    ##################################

    mix_list = args.Hmix
    DB_DICT = {} 
    DB_DICT["frac"]    = []
    DB_DICT["mixH"]    = []
    DB_DICT["d"]       = []
    DB_DICT["nu_mean"] = []
    DB_DICT["nu_r2"  ] = []
    y_dict     = {}
    r2_dict    = {}
    ntraj_dict = {}
    dop            = args.dop
    coords_files   = args.c
    nproc          = args.nproc
    starting_index = 0

    fig = plt.figure( figsize=(8,6) )
    ax  = plt.axes() 
    ax.tick_params(direction='in', bottom=True, top=True, left=True, right=True, which='both')
    ax.tick_params(axis='x', labelsize=16)
    ax.tick_params(axis='y', labelsize=16)
    i = 0

    # instantiating pool
    pool1 = multiprocessing.Pool ( processes=nproc )
    pool_list = [pool1]
    print ("mix_list = ",mix_list)
    xx = 0
    for mixH in mix_list:
        fig = plt.figure(num=xx, figsize=(8,8) )
        ax  = plt.axes() 
        ax.tick_params(direction='in', bottom=True, top=True, left=True, right=True, which='both')
        ax.tick_params(axis='x', labelsize=16)
        ax.tick_params(axis='y', labelsize=16)
        xx += 1
        U_list = [ "U"+str(j) for j in range(1,12) ]

        # get num_list for each temperature
        master_U_list  = []
        master_num_list  = []
        flory_mean = []
        flory_err  = []
        flory_r2   = []
        r2_dict.clear()
        ntraj_dict.clear()
        for U in U_list:

            num_list = list(np.unique ( aux.dir2nsim (os.listdir (U + "/DOP_" + str(dop) + "/" + mixH ) ) ) )
            master_num_list.extend ( num_list )
            master_U_list.extend ( [U]*len( num_list ) )
            ntraj_dict[U] = len ( num_list )
            y_dict [U]    = []
            r2_dict[U]    = []

        # start multiprocessing... keeping in mind that each node only has 96 cores
        # start splitting up master_num_list and master_temp_list
        idx_range = len (master_num_list)//nproc + 1

        for uidx in range(idx_range):
            if uidx == idx_range-1:
                results = pool_list[ 0 ] .starmap ( get_avg_amounts, zip( master_U_list[uidx*nproc:], itertools.repeat(mixH), master_num_list[uidx*nproc:], itertools.repeat(dop), itertools.repeat(coords_files), itertools.repeat(starting_index), itertools.repeat(args.d1), itertools.repeat(args.d2) ) )
            else:
                results = pool_list[ 0 ] .starmap ( get_avg_amounts, zip( master_U_list[uidx*nproc:], itertools.repeat(mixH),  master_num_list[uidx*nproc:(uidx+1)*nproc], itertools.repeat(dop), itertools.repeat(coords_files), itertools.repeat(starting_index), itertools.repeat(args.d1), itertools.repeat(args.d2) ) )

            print ("Pool has been closed. This pool has {} threads.".format (len(results) ), flush=True )
            for k in range(len(master_U_list[uidx*nproc:(uidx+1)*nproc])):
                y_dict[master_U_list[uidx*nproc+k]].append ( results[k] )


        for U in np.unique (master_U_list):
            frac = float(aux.get_frac (U+"/geom_and_esurf.txt"))
            y_dict[U] = np.array (y_dict[U])
            y = np.mean (y_dict[U], axis=0)
            y = np.asarray (np.log(y))
            x = np.arange (args.d1, args.d2+1, 1)
            x = np.asarray (np.log(x)).reshape((-1,1))
            ax.plot (x, y, marker='o', linestyle='-', markeredgecolor='k')
            for j in range (1, len(x)-1):
                model = HuberRegressor()
                model.fit(x[j-1:j+2], y[j-1:j+2])
                r2 = model.score (x[j-1:j+2], y[j-1:j+2])
                flory_mean.append ( model.coef_[0] )
                flory_r2.append ( r2 )
                DB_DICT["frac"].append (frac)
                DB_DICT["mixH"].append (mixH)
                DB_DICT["d"].append (j+1)
                DB_DICT["nu_mean"].append (model.coef_[0])
                DB_DICT["nu_r2"].append (r2)
        plt.savefig ("plot_U_"+str(U)+"_"+str(args.d1)+"_"+str(args.d2)+"set_" + str(args.set) + ".png", dpi=1200, bbox_inches='tight')

    pool1.close()
    pool1.join()

    i=0
    df = pd.DataFrame.from_dict (DB_DICT)
    df.to_csv ("FLORY-EXPONENTS-"+str(args.d1)+"-"+str(args.d2)+"_set_"+str(args.set)+".mc", sep='|', index=False)

    stop = time.time()

    print ("Run time for N = " + str(args.dop) + " is {:.2f} seconds.".format(stop-start), flush=True)

